{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook aims to discover some potential patterns by analyzing the content of each post with the help of Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class apriori:\n",
    "    def __init__(self, min_support, min_confidence):\n",
    "        self.sets_count = dict()\n",
    "        self.min_support_sets = None\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "    \n",
    "    def get_set_count(self, data, set_candidate):\n",
    "        '''\n",
    "        Return number of occurrence of set_candidate in the data\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "        \n",
    "        set_candidate: Set\n",
    "            A set of items to be counted\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Int:\n",
    "            Number of set_candidate in data\n",
    "        '''  \n",
    "        item_key = tuple(set_candidate)\n",
    "        if item_key not in self.sets_count:\n",
    "            item_count = 0\n",
    "            for record in data:\n",
    "                if set_candidate.issubset(record):\n",
    "                    item_count += 1\n",
    "            self.sets_count[item_key] = item_count\n",
    "        return self.sets_count[item_key]\n",
    "    \n",
    "    def gen_first_sets(self, data):\n",
    "        '''\n",
    "        Return lists of candidate sets. Each set has one item\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        List:\n",
    "            A list of sets with length one\n",
    "        '''  \n",
    "        res = set()\n",
    "        for record in data:\n",
    "            res.update(record)\n",
    "        res = [{i} for i in res]\n",
    "        return res\n",
    "    \n",
    "    def gen_CTable(self, data, subsets):\n",
    "        '''\n",
    "        Return support_value for each item in subsets which is greater than min_support\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "        \n",
    "        subsets: List\n",
    "            A list of items that need to be counted. Each item should a set\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict: \n",
    "            A dictionary mapping items in subsets and corresponding support value.\n",
    "            format: {tuple : float}\n",
    "        '''     \n",
    "        CTable = defaultdict(lambda: 0)\n",
    "        for item in subsets:\n",
    "            CTable[tuple(item)] = self.get_set_count(data, item)\n",
    "        l = len(data)\n",
    "        for k in list(CTable.keys()):\n",
    "            if CTable[k]/l < self.min_support:\n",
    "                CTable.pop(k)\n",
    "            else:\n",
    "                CTable[k] /= l\n",
    "        return CTable\n",
    "    \n",
    "    def is_super_set(self, candidate_set, recorded_sets):\n",
    "        '''\n",
    "        If any subset of candidate_set is in recorded_sets, return True.\n",
    "        e.g. candidate_set = {1,2,3}   recorded_sets = {(5,7), (9,), (1,3)}   Return: True \n",
    "        Because {1,3} is one of subset of {1,2,3} and it is in recorded_sets\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        candidate_set: set\n",
    "            The set to be tested\n",
    "        \n",
    "        recorded_sets: Set\n",
    "            Set of tuples\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Bool:\n",
    "            Whether candidate_set is super set of any set in recorded_sets\n",
    "        '''\n",
    "        l = len(candidate_set)\n",
    "        for k in range(1, l):\n",
    "            sub_candidates = list(itertools.combinations(candidate_set, k))\n",
    "            for sub_candidate in sub_candidates:\n",
    "                if sub_candidate in recorded_sets:\n",
    "                    return True\n",
    "        return False\n",
    " \n",
    "    \n",
    "    def gen_k_sets(self, prev_candidates, k):\n",
    "        '''\n",
    "        Return lists of candidate sets. Each set has k items\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        prev_candidates: set\n",
    "            Set of tuples of satisfying min_support\n",
    "        \n",
    "        k: int\n",
    "            Specify the length of sets generated\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        List:\n",
    "            A list of sets with length k\n",
    "        '''\n",
    "        c_prev = [set(i) for i in prev_candidates]  # convert to list of sets\n",
    "        c_next = list(itertools.combinations(c_prev, 2)) # list of tuples of 2 sets\n",
    "        c_next = {tuple(set1.union(set2)) for set1, set2 in c_next if len(tuple(set1.union(set2))) == k}  # convert to set of tuples with length k\n",
    "        # all subsets with length k-1 of candidate should fulfill min_support\n",
    "        remove_list = []\n",
    "        for candidate in c_next:\n",
    "            sub_candidates = itertools.combinations(candidate, k-1)\n",
    "            for sub_candidate in sub_candidates:\n",
    "                if sub_candidate not in prev_candidates:\n",
    "                    remove_list.append(candidate)\n",
    "                    break\n",
    "        for candidate in remove_list:\n",
    "            c_next.remove(candidate)\n",
    "        c_next = [set(i) for i in c_next]\n",
    "        return c_next\n",
    "    \n",
    "    def get_min_support_sets(self, data):\n",
    "        '''\n",
    "        Return subsets with support greater than min_support\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict:\n",
    "            A dictionary mapping subsets and corresponding support values\n",
    "        '''\n",
    "        # Generate count table for set with length 1\n",
    "        c_dict = dict()    # c_dict[k] store the CTable of length k\n",
    "        subsets = self.gen_first_sets(data)\n",
    "        ck = self.gen_CTable(data, subsets)\n",
    "        c_dict[1] = ck\n",
    "        k=2\n",
    "        while len(c_dict[k-1]) > 1:\n",
    "            subsets = self.gen_k_sets(set(c_dict[k-1].keys()), k)\n",
    "            ck = self.gen_CTable(data, subsets)\n",
    "            c_dict[k] = ck\n",
    "            k+=1\n",
    "        self.min_support_sets = c_dict\n",
    "        return self.min_support_sets\n",
    "    \n",
    "    def get_confidence(self, data, left_set, right_set):\n",
    "        '''\n",
    "        Return confidence value for the rule 'left_set -> right_set'\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "        \n",
    "        left_set: set\n",
    "            A set of items on left hand side of rule\n",
    "        \n",
    "        right_set: set\n",
    "            A set of items on right hand side of rule\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Float:\n",
    "            Confidence value of the rule\n",
    "        '''\n",
    "        left_count = self.get_set_count(data, left_set)\n",
    "        combined_count = self.get_set_count(data, left_set.union(right_set))\n",
    "        return combined_count/left_count\n",
    "        \n",
    "        \n",
    "    def find_rules(self, data):\n",
    "        '''\n",
    "        Return rules with confidence value greater than min_confidence\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict:\n",
    "            A dictionary mapping rules and corresponding confidence values\n",
    "        '''\n",
    "        hist = defaultdict(lambda: set())\n",
    "        res = dict()\n",
    "        l = len(data)\n",
    "        if self.min_support_sets is None:\n",
    "            self.get_min_support_sets(data)\n",
    "        for k in self.min_support_sets.keys():\n",
    "            if k == 1:\n",
    "                continue\n",
    "            for subset in self.min_support_sets[k].keys():\n",
    "                for num_left in range(1,k):   # num_left is number of items on left hand side of the rule \n",
    "                    num_right = k - num_left\n",
    "                    left_candidates = list(itertools.combinations(subset, num_left))\n",
    "                    right_candidates = list(itertools.combinations(subset, num_right))\n",
    "                    # for each combination of rules\n",
    "                    for left_candidate in left_candidates:\n",
    "                        for right_candidate in right_candidates:\n",
    "                            if set(left_candidate).issubset(set(right_candidate)) or set(right_candidate).issubset(set(left_candidate)):\n",
    "                                continue\n",
    "                            right_candidate = tuple(set(right_candidate) - set(left_candidate))\n",
    "                            # Given left_candidate, if right_candidate is superset of a 'past right_candidate' with confidence value smaller than required value,\n",
    "                            # then its confidence value must be smaller than required value\n",
    "                            if self.is_super_set(set(right_candidate), hist[left_candidate]):\n",
    "                                hist[left_candidate].add(right_candidate)\n",
    "                                continue\n",
    "                            else:\n",
    "                                confidence_val = self.get_confidence(data, set(left_candidate), set(right_candidate))\n",
    "                                support = self.get_set_count(data, set(left_candidate).union(set(right_candidate)))/l\n",
    "                                if confidence_val >= self.min_confidence:\n",
    "                                    str_output = '{} -> {}'.format(left_candidate, right_candidate)\n",
    "                                    if str_output not in res:\n",
    "                                        print('Rule: {}  Support: {}  Confidence: {}'.format(str_output, support, confidence_val))\n",
    "                                        res[str_output] = confidence_val\n",
    "                                else:\n",
    "                                    hist[left_candidate].add(right_candidate)\n",
    "        return res\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Post_datetime</th>\n",
       "      <th>Content</th>\n",
       "      <th>Video_link(expired)</th>\n",
       "      <th>Repost_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Video_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ice-dance柳鑫宇</td>\n",
       "      <td>2022-03-05 16:10:00.000000</td>\n",
       "      <td>我用了我很喜欢的一首歌曲来当背景音乐，来与大家分享我的闭幕式vlog，北京冬奥会真的结束了，...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/b7tMvrhxlx07Uf...</td>\n",
       "      <td>939</td>\n",
       "      <td>1532</td>\n",
       "      <td>29070</td>\n",
       "      <td>#冬奥会# #冬奥隔离日记#</td>\n",
       "      <td>./video/1.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>奥林匹克运动会</td>\n",
       "      <td>2022-03-16 12:30:00.000001</td>\n",
       "      <td>一起欣赏 @金博洋的天天  在北京2022年冬奥会上的短节目表演   #奥运会#  ｜  #...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/Due4Y9s7lx07Uw...</td>\n",
       "      <td>852</td>\n",
       "      <td>305</td>\n",
       "      <td>2561</td>\n",
       "      <td>#奥运会# #北京2022年冬奥会#</td>\n",
       "      <td>./video/2.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥林匹克运动会</td>\n",
       "      <td>2022-03-18 08:00:00.000001</td>\n",
       "      <td>一起回顾 #羽生结弦# 的冬奥之旅   #奥运会#  ｜ #北京2022年冬奥会#    L...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/sJtalMgylx07Uy...</td>\n",
       "      <td>3565</td>\n",
       "      <td>851</td>\n",
       "      <td>19006</td>\n",
       "      <td>#羽生结弦# #奥运会# #北京2022年冬奥会#</td>\n",
       "      <td>./video/3.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ice-dance柳鑫宇</td>\n",
       "      <td>2022-02-23 14:21:00.000001</td>\n",
       "      <td>隔离中也要运动起来哦！ 训练计划安排！ 大家快跟我一起操练起来吧！   #冬奥会#  #冬奥...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/DVYNperQlx07TY...</td>\n",
       "      <td>285</td>\n",
       "      <td>1009</td>\n",
       "      <td>15334</td>\n",
       "      <td>#冬奥会# #冬奥隔离日记#</td>\n",
       "      <td>./video/4.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sy王诗玥</td>\n",
       "      <td>2022-02-23 14:21:00.000001</td>\n",
       "      <td>隔离期间也要动起来～  大家跟我一起锻炼吧～🤸🏻‍♀️   #冬奥会#  #冬奥隔离日记# ...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/E5AJHWkMlx07TY...</td>\n",
       "      <td>288</td>\n",
       "      <td>978</td>\n",
       "      <td>13871</td>\n",
       "      <td>#冬奥会# #冬奥隔离日记#</td>\n",
       "      <td>./video/5.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Username              Post_datetime  \\\n",
       "0  Ice-dance柳鑫宇 2022-03-05 16:10:00.000000   \n",
       "1       奥林匹克运动会 2022-03-16 12:30:00.000001   \n",
       "2       奥林匹克运动会 2022-03-18 08:00:00.000001   \n",
       "3  Ice-dance柳鑫宇 2022-02-23 14:21:00.000001   \n",
       "4         sy王诗玥 2022-02-23 14:21:00.000001   \n",
       "\n",
       "                                             Content  \\\n",
       "0  我用了我很喜欢的一首歌曲来当背景音乐，来与大家分享我的闭幕式vlog，北京冬奥会真的结束了，...   \n",
       "1  一起欣赏 @金博洋的天天  在北京2022年冬奥会上的短节目表演   #奥运会#  ｜  #...   \n",
       "2  一起回顾 #羽生结弦# 的冬奥之旅   #奥运会#  ｜ #北京2022年冬奥会#    L...   \n",
       "3  隔离中也要运动起来哦！ 训练计划安排！ 大家快跟我一起操练起来吧！   #冬奥会#  #冬奥...   \n",
       "4  隔离期间也要动起来～  大家跟我一起锻炼吧～🤸🏻‍♀️   #冬奥会#  #冬奥隔离日记# ...   \n",
       "\n",
       "                                 Video_link(expired)  Repost_Count  \\\n",
       "0  https://f.video.weibocdn.com/o0/b7tMvrhxlx07Uf...           939   \n",
       "1  https://f.video.weibocdn.com/o0/Due4Y9s7lx07Uw...           852   \n",
       "2  https://f.video.weibocdn.com/o0/sJtalMgylx07Uy...          3565   \n",
       "3  https://f.video.weibocdn.com/o0/DVYNperQlx07TY...           285   \n",
       "4  https://f.video.weibocdn.com/o0/E5AJHWkMlx07TY...           288   \n",
       "\n",
       "   Comment_Count  Like_Count                   Keywords     Video_file  \n",
       "0           1532       29070             #冬奥会# #冬奥隔离日记#  ./video/1.mp4  \n",
       "1            305        2561         #奥运会# #北京2022年冬奥会#  ./video/2.mp4  \n",
       "2            851       19006  #羽生结弦# #奥运会# #北京2022年冬奥会#  ./video/3.mp4  \n",
       "3           1009       15334             #冬奥会# #冬奥隔离日记#  ./video/4.mp4  \n",
       "4            978       13871             #冬奥会# #冬奥隔离日记#  ./video/5.mp4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_excel('./data.xlsx')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify frequent phrases of differene length\n",
    "def n_words_count(data, n):\n",
    "        '''\n",
    "        Return number of occurrence of words with length n.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list sentences or paragraphs\n",
    "        \n",
    "        n: int\n",
    "            return the words with length n\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict:\n",
    "            A dictionary mapping words with their frequency\n",
    "        '''\n",
    "        count_dict = defaultdict(lambda: 0)\n",
    "        for sentence in data:\n",
    "            \n",
    "            for ptr1 in range(len(sentence)-n):\n",
    "                ptr2 = ptr1 + n\n",
    "                phrases = sentence[ptr1:ptr2]\n",
    "                count_dict[phrases]+=1\n",
    "        pairs = list(count_dict.items())\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])[::-1]\n",
    "        return pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output frequency of word phrases with length 2 to 10\n",
    "# Used to select meaningful frequent word phrase\n",
    "data = list(df_all['Content'].values)\n",
    "unwanted = re.compile('[!，?#：（）。《》、@……&“”！【】｜....____↓？ ]')\n",
    "data = [unwanted.sub('', s) for s in data]\n",
    "for word_l in range(2, 11):\n",
    "    words_count = n_words_count(data, word_l)\n",
    "    df = pd.DataFrame(words_count, columns=['words','count'])\n",
    "    df.to_excel('./data/word_count_tables/wc{}.xlsx'.format(word_l), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>残奥</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>开幕</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>跳台滑雪</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>金牌</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>闭幕</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words  count\n",
       "0    残奥    991\n",
       "1    开幕    265\n",
       "2  跳台滑雪    116\n",
       "3    金牌    183\n",
       "4    闭幕    179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words = pd.read_excel(r'./data/freq_words.xlsx')\n",
    "freq_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Username: Ice-dance柳鑫宇', '闭幕'}, {'Username: 奥林匹克运动会'}, {'Username: 奥林匹克运动会', '羽生结弦'}, {'Username: Ice-dance柳鑫宇'}, {'Username: sy王诗玥'}]\n"
     ]
    }
   ],
   "source": [
    "# Turn each post into a bag of features so that it can be passed to Apriori \n",
    "data = []\n",
    "for idx, row in df_all.iterrows():\n",
    "    bag = set()\n",
    "    bag.add('Username: '+row['Username'])\n",
    "    if not row['Keywords']:\n",
    "        keywords = {kw for kw in row['Keywords'].split()}\n",
    "        bag.update(keywords)\n",
    "    text = row['Content']\n",
    "    for interested_word in freq_words['words']:\n",
    "        if interested_word in text:\n",
    "            bag.add(interested_word)\n",
    "    data.append(bag)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: ('冰立方',) -> ('冰壶',)  Support: 0.02185792349726776  Confidence: 0.7058823529411765\n",
      "Rule: ('雪车雪橇',) -> ('延庆',)  Support: 0.026411657559198543  Confidence: 0.6444444444444445\n",
      "Rule: ('短道速滑',) -> ('Username: 北京2022年冬奥会',)  Support: 0.033697632058287796  Confidence: 0.6379310344827587\n",
      "Rule: ('人民日报',) -> ('Username: 人民日报',)  Support: 0.02185792349726776  Confidence: 0.6\n",
      "Rule: ('速度滑冰',) -> ('Username: 北京2022年冬奥会',)  Support: 0.03278688524590164  Confidence: 0.6792452830188679\n",
      "Rule: ('冰丝带',) -> ('速度滑冰',)  Support: 0.02185792349726776  Confidence: 0.6666666666666666\n",
      "Rule: ('闭幕',) -> ('残奥',)  Support: 0.05009107468123861  Confidence: 0.6179775280898876\n",
      "Rule: ('鸟巢',) -> ('开幕',)  Support: 0.030054644808743168  Confidence: 0.7333333333333333\n",
      "Rule: ('Username: 央视频',) -> ('残奥',)  Support: 0.033697632058287796  Confidence: 0.6379310344827587\n",
      "Rule: ('雪容融',) -> ('残奥',)  Support: 0.061930783242258654  Confidence: 0.7816091954022989\n",
      "Rule: ('冰壶',) -> ('残奥',)  Support: 0.04918032786885246  Confidence: 0.6352941176470588\n",
      "Rule: ('云顶滑雪公园',) -> ('张家口',)  Support: 0.02459016393442623  Confidence: 0.675\n",
      "Rule: ('国家速滑馆',) -> ('冰丝带',)  Support: 0.023679417122040074  Confidence: 0.7428571428571429\n",
      "Rule: ('冰丝带',) -> ('国家速滑馆',)  Support: 0.023679417122040074  Confidence: 0.7222222222222222\n",
      "Rule: ('3月4日',) -> ('开幕',)  Support: 0.030054644808743168  Confidence: 0.7021276595744681\n",
      "Rule: ('3月4日',) -> ('残奥',)  Support: 0.042805100182149364  Confidence: 1.0\n",
      "Rule: ('单板滑雪', '高山滑雪') -> ('残奥',)  Support: 0.02185792349726776  Confidence: 0.6\n",
      "Rule: ('短道速滑', '金牌') -> ('Username: 北京2022年冬奥会',)  Support: 0.020036429872495445  Confidence: 0.88\n",
      "Rule: ('越野滑雪', '单板滑雪') -> ('残奥',)  Support: 0.023679417122040074  Confidence: 0.7428571428571429\n",
      "Rule: ('残奥', '冰球') -> ('单板滑雪',)  Support: 0.030054644808743168  Confidence: 0.673469387755102\n",
      "Rule: ('残奥', '单板滑雪') -> ('冰球',)  Support: 0.030054644808743168  Confidence: 0.66\n",
      "Rule: ('冰球', '单板滑雪') -> ('残奥',)  Support: 0.030054644808743168  Confidence: 1.0\n",
      "Rule: ('冰球', '越野滑雪') -> ('残奥',)  Support: 0.020947176684881604  Confidence: 1.0\n",
      "Rule: ('冰球', '单板滑雪') -> ('高山滑雪',)  Support: 0.02185792349726776  Confidence: 0.7272727272727273\n",
      "Rule: ('冰球', '高山滑雪') -> ('单板滑雪',)  Support: 0.02185792349726776  Confidence: 0.96\n",
      "Rule: ('单板滑雪', '高山滑雪') -> ('冰球',)  Support: 0.02185792349726776  Confidence: 0.6\n",
      "Rule: ('3月4日', '冰球') -> ('残奥',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('3月4日', '单板滑雪') -> ('残奥',)  Support: 0.023679417122040074  Confidence: 1.0\n",
      "Rule: ('残奥', '冰墩墩') -> ('雪容融',)  Support: 0.0273224043715847  Confidence: 1.0\n",
      "Rule: ('冰墩墩', '雪容融') -> ('残奥',)  Support: 0.0273224043715847  Confidence: 0.6976744186046512\n",
      "Rule: ('3月4日', '冰壶') -> ('残奥',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('冰壶', '冰球') -> ('高山滑雪',)  Support: 0.020036429872495445  Confidence: 0.6470588235294118\n",
      "Rule: ('冰壶', '高山滑雪') -> ('冰球',)  Support: 0.020036429872495445  Confidence: 0.9565217391304348\n",
      "Rule: ('冰球', '高山滑雪') -> ('冰壶',)  Support: 0.020036429872495445  Confidence: 0.88\n",
      "Rule: ('Username: 北京2022年冬奥会', '雪容融') -> ('残奥',)  Support: 0.020947176684881604  Confidence: 0.9583333333333334\n",
      "Rule: ('3月4日', '冰壶') -> ('冰球',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('3月4日', '冰球') -> ('冰壶',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('冰壶', '冰球') -> ('3月4日',)  Support: 0.02459016393442623  Confidence: 0.7941176470588235\n",
      "Rule: ('越野滑雪', '单板滑雪') -> ('高山滑雪',)  Support: 0.02459016393442623  Confidence: 0.7714285714285715\n",
      "Rule: ('越野滑雪', '高山滑雪') -> ('单板滑雪',)  Support: 0.02459016393442623  Confidence: 0.84375\n",
      "Rule: ('单板滑雪', '高山滑雪') -> ('越野滑雪',)  Support: 0.02459016393442623  Confidence: 0.675\n",
      "Rule: ('3月4日', '冰球') -> ('单板滑雪',)  Support: 0.023679417122040074  Confidence: 0.9629629629629629\n",
      "Rule: ('3月4日', '单板滑雪') -> ('冰球',)  Support: 0.023679417122040074  Confidence: 1.0\n",
      "Rule: ('冰球', '单板滑雪') -> ('3月4日',)  Support: 0.023679417122040074  Confidence: 0.7878787878787878\n",
      "Rule: ('3月4日', '冰壶') -> ('单板滑雪',)  Support: 0.023679417122040074  Confidence: 0.9629629629629629\n",
      "Rule: ('3月4日', '单板滑雪') -> ('冰壶',)  Support: 0.023679417122040074  Confidence: 1.0\n",
      "Rule: ('冰壶', '单板滑雪') -> ('3月4日',)  Support: 0.023679417122040074  Confidence: 0.9629629629629629\n",
      "Rule: ('冰壶', '冰球') -> ('单板滑雪',)  Support: 0.02459016393442623  Confidence: 0.7941176470588235\n",
      "Rule: ('冰壶', '单板滑雪') -> ('冰球',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('冰球', '单板滑雪') -> ('冰壶',)  Support: 0.02459016393442623  Confidence: 0.8181818181818182\n",
      "Rule: ('越野滑雪', '高山滑雪') -> ('残奥',)  Support: 0.020947176684881604  Confidence: 0.71875\n",
      "Rule: ('冰球', '高山滑雪') -> ('残奥',)  Support: 0.02185792349726776  Confidence: 0.96\n"
     ]
    }
   ],
   "source": [
    "model = apriori(min_support=0.02, min_confidence=0.6)\n",
    "rules = model.find_rules(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not all rules are meaningful, some contain interesting facts. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule: ('雪容融',) -> ('残奥',)  Support: 0.061930783242258654  Confidence: 0.7816091954022989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule relates the name of mascot ('雪容融') to Winter Paralympic Games ('残奥'). The confidence value tells that given that the word '雪容融' appears in the post content, there is around 78.16% of chance that the word '残奥' will also appear in the same post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
