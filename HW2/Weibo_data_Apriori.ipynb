{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook aims to discover some potential patterns by analyzing the content of each post with the help of Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class apriori:\n",
    "    def __init__(self, min_support, min_confidence):\n",
    "        self.sets_count = dict()\n",
    "        self.min_support_sets = None\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "    \n",
    "    def get_set_count(self, data, set_candidate):\n",
    "        '''\n",
    "        Return number of occurrence of set_candidate in the data\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "        \n",
    "        set_candidate: Set\n",
    "            A set of items to be counted\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Int:\n",
    "            Number of set_candidate in data\n",
    "        '''  \n",
    "        item_key = tuple(set_candidate)\n",
    "        if item_key not in self.sets_count:\n",
    "            item_count = 0\n",
    "            for record in data:\n",
    "                if set_candidate.issubset(record):\n",
    "                    item_count += 1\n",
    "            self.sets_count[item_key] = item_count\n",
    "        return self.sets_count[item_key]\n",
    "    \n",
    "    def gen_first_sets(self, data):\n",
    "        '''\n",
    "        Return lists of candidate sets. Each set has one item\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        List:\n",
    "            A list of sets with length one\n",
    "        '''  \n",
    "        res = set()\n",
    "        for record in data:\n",
    "            res.update(record)\n",
    "        res = [{i} for i in res]\n",
    "        return res\n",
    "    \n",
    "    def gen_CTable(self, data, subsets):\n",
    "        '''\n",
    "        Return support_value for each item in subsets which is greater than min_support\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "        \n",
    "        subsets: List\n",
    "            A list of items that need to be counted. Each item should a set\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict: \n",
    "            A dictionary mapping items in subsets and corresponding support value.\n",
    "            format: {tuple : float}\n",
    "        '''     \n",
    "        CTable = defaultdict(lambda: 0)\n",
    "        for item in subsets:\n",
    "            CTable[tuple(item)] = self.get_set_count(data, item)\n",
    "        l = len(data)\n",
    "        for k in list(CTable.keys()):\n",
    "            if CTable[k]/l < self.min_support:\n",
    "                CTable.pop(k)\n",
    "            else:\n",
    "                CTable[k] /= l\n",
    "        return CTable\n",
    "    \n",
    "    def is_super_set(self, candidate_set, recorded_sets):\n",
    "        '''\n",
    "        If any subset of candidate_set is in recorded_sets, return True.\n",
    "        e.g. candidate_set = {1,2,3}   recorded_sets = {(5,7), (9,), (1,3)}   Return: True \n",
    "        Because {1,3} is one of subset of {1,2,3} and it is in recorded_sets\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        candidate_set: set\n",
    "            The set to be tested\n",
    "        \n",
    "        recorded_sets: Set\n",
    "            Set of tuples\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Bool:\n",
    "            Whether candidate_set is super set of any set in recorded_sets\n",
    "        '''\n",
    "        l = len(candidate_set)\n",
    "        for k in range(1, l):\n",
    "            sub_candidates = list(itertools.combinations(candidate_set, k))\n",
    "            for sub_candidate in sub_candidates:\n",
    "                if sub_candidate in recorded_sets:\n",
    "                    return True\n",
    "        return False\n",
    " \n",
    "    \n",
    "    def gen_k_sets(self, prev_candidates, k):\n",
    "        '''\n",
    "        Return lists of candidate sets. Each set has k items\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        prev_candidates: set\n",
    "            Set of tuples of satisfying min_support\n",
    "        \n",
    "        k: int\n",
    "            Specify the length of sets generated\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        List:\n",
    "            A list of sets with length k\n",
    "        '''\n",
    "        c_prev = [set(i) for i in prev_candidates]  # convert to list of sets\n",
    "        c_next = list(itertools.combinations(c_prev, 2)) # list of tuples of 2 sets\n",
    "        c_next = {tuple(set1.union(set2)) for set1, set2 in c_next if len(tuple(set1.union(set2))) == k}  # convert to set of tuples with length k\n",
    "        # all subsets with length k-1 of candidate should fulfill min_support\n",
    "        remove_list = []\n",
    "        for candidate in c_next:\n",
    "            sub_candidates = itertools.combinations(candidate, k-1)\n",
    "            for sub_candidate in sub_candidates:\n",
    "                if sub_candidate not in prev_candidates:\n",
    "                    remove_list.append(candidate)\n",
    "                    break\n",
    "        for candidate in remove_list:\n",
    "            c_next.remove(candidate)\n",
    "        c_next = [set(i) for i in c_next]\n",
    "        return c_next\n",
    "    \n",
    "    def get_min_support_sets(self, data):\n",
    "        '''\n",
    "        Return subsets with support greater than min_support\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict:\n",
    "            A dictionary mapping subsets and corresponding support values\n",
    "        '''\n",
    "        # Generate count table for set with length 1\n",
    "        c_dict = dict()    # c_dict[k] store the CTable of length k\n",
    "        subsets = self.gen_first_sets(data)\n",
    "        ck = self.gen_CTable(data, subsets)\n",
    "        c_dict[1] = ck\n",
    "        k=2\n",
    "        while len(c_dict[k-1]) > 1:\n",
    "            subsets = self.gen_k_sets(set(c_dict[k-1].keys()), k)\n",
    "            ck = self.gen_CTable(data, subsets)\n",
    "            c_dict[k] = ck\n",
    "            k+=1\n",
    "        self.min_support_sets = c_dict\n",
    "        return self.min_support_sets\n",
    "    \n",
    "    def get_confidence(self, data, left_set, right_set):\n",
    "        '''\n",
    "        Return confidence value for the rule 'left_set -> right_set'\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "        \n",
    "        left_set: set\n",
    "            A set of items on left hand side of rule\n",
    "        \n",
    "        right_set: set\n",
    "            A set of items on right hand side of rule\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Float:\n",
    "            Confidence value of the rule\n",
    "        '''\n",
    "        left_count = self.get_set_count(data, left_set)\n",
    "        combined_count = self.get_set_count(data, left_set.union(right_set))\n",
    "        return combined_count/left_count\n",
    "        \n",
    "        \n",
    "    def find_rules(self, data):\n",
    "        '''\n",
    "        Return rules with confidence value greater than min_confidence\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list of records. Each records should be a set.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict:\n",
    "            A dictionary mapping rules and corresponding confidence values\n",
    "        '''\n",
    "        hist = defaultdict(lambda: set())\n",
    "        res = dict()\n",
    "        l = len(data)\n",
    "        if self.min_support_sets is None:\n",
    "            self.get_min_support_sets(data)\n",
    "        for k in self.min_support_sets.keys():\n",
    "            if k == 1:\n",
    "                continue\n",
    "            for subset in self.min_support_sets[k].keys():\n",
    "                for num_left in range(1,k):   # num_left is number of items on left hand side of the rule \n",
    "                    num_right = k - num_left\n",
    "                    left_candidates = list(itertools.combinations(subset, num_left))\n",
    "                    right_candidates = list(itertools.combinations(subset, num_right))\n",
    "                    # for each combination of rules\n",
    "                    for left_candidate in left_candidates:\n",
    "                        for right_candidate in right_candidates:\n",
    "                            if set(left_candidate).issubset(set(right_candidate)) or set(right_candidate).issubset(set(left_candidate)):\n",
    "                                continue\n",
    "                            right_candidate = tuple(set(right_candidate) - set(left_candidate))\n",
    "                            # Given left_candidate, if right_candidate is superset of a 'past right_candidate' with confidence value smaller than required value,\n",
    "                            # then its confidence value must be smaller than required value\n",
    "                            if self.is_super_set(set(right_candidate), hist[left_candidate]):\n",
    "                                hist[left_candidate].add(right_candidate)\n",
    "                                continue\n",
    "                            else:\n",
    "                                confidence_val = self.get_confidence(data, set(left_candidate), set(right_candidate))\n",
    "                                support = self.get_set_count(data, set(left_candidate).union(set(right_candidate)))/l\n",
    "                                if confidence_val >= self.min_confidence:\n",
    "                                    str_output = '{} -> {}'.format(left_candidate, right_candidate)\n",
    "                                    if str_output not in res:\n",
    "                                        print('Rule: {}  Support: {}  Confidence: {}'.format(str_output, support, confidence_val))\n",
    "                                        res[str_output] = confidence_val\n",
    "                                else:\n",
    "                                    hist[left_candidate].add(right_candidate)\n",
    "        return res\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Post_datetime</th>\n",
       "      <th>Content</th>\n",
       "      <th>Video_link(expired)</th>\n",
       "      <th>Repost_Count</th>\n",
       "      <th>Comment_Count</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Video_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ice-danceæŸ³é‘«å®‡</td>\n",
       "      <td>2022-03-05 16:10:00.000000</td>\n",
       "      <td>æˆ‘ç”¨äº†æˆ‘å¾ˆå–œæ¬¢çš„ä¸€é¦–æ­Œæ›²æ¥å½“èƒŒæ™¯éŸ³ä¹ï¼Œæ¥ä¸å¤§å®¶åˆ†äº«æˆ‘çš„é—­å¹•å¼vlogï¼ŒåŒ—äº¬å†¬å¥¥ä¼šçœŸçš„ç»“æŸäº†ï¼Œ...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/b7tMvrhxlx07Uf...</td>\n",
       "      <td>939</td>\n",
       "      <td>1532</td>\n",
       "      <td>29070</td>\n",
       "      <td>#å†¬å¥¥ä¼š# #å†¬å¥¥éš”ç¦»æ—¥è®°#</td>\n",
       "      <td>./video/1.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>å¥¥æ—åŒ¹å…‹è¿åŠ¨ä¼š</td>\n",
       "      <td>2022-03-16 12:30:00.000001</td>\n",
       "      <td>ä¸€èµ·æ¬£èµ @é‡‘åšæ´‹çš„å¤©å¤©  åœ¨åŒ—äº¬2022å¹´å†¬å¥¥ä¼šä¸Šçš„çŸ­èŠ‚ç›®è¡¨æ¼”   #å¥¥è¿ä¼š#  ï½œ  #...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/Due4Y9s7lx07Uw...</td>\n",
       "      <td>852</td>\n",
       "      <td>305</td>\n",
       "      <td>2561</td>\n",
       "      <td>#å¥¥è¿ä¼š# #åŒ—äº¬2022å¹´å†¬å¥¥ä¼š#</td>\n",
       "      <td>./video/2.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>å¥¥æ—åŒ¹å…‹è¿åŠ¨ä¼š</td>\n",
       "      <td>2022-03-18 08:00:00.000001</td>\n",
       "      <td>ä¸€èµ·å›é¡¾ #ç¾½ç”Ÿç»“å¼¦# çš„å†¬å¥¥ä¹‹æ—…   #å¥¥è¿ä¼š#  ï½œ #åŒ—äº¬2022å¹´å†¬å¥¥ä¼š#    L...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/sJtalMgylx07Uy...</td>\n",
       "      <td>3565</td>\n",
       "      <td>851</td>\n",
       "      <td>19006</td>\n",
       "      <td>#ç¾½ç”Ÿç»“å¼¦# #å¥¥è¿ä¼š# #åŒ—äº¬2022å¹´å†¬å¥¥ä¼š#</td>\n",
       "      <td>./video/3.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ice-danceæŸ³é‘«å®‡</td>\n",
       "      <td>2022-02-23 14:21:00.000001</td>\n",
       "      <td>éš”ç¦»ä¸­ä¹Ÿè¦è¿åŠ¨èµ·æ¥å“¦ï¼ è®­ç»ƒè®¡åˆ’å®‰æ’ï¼ å¤§å®¶å¿«è·Ÿæˆ‘ä¸€èµ·æ“ç»ƒèµ·æ¥å§ï¼   #å†¬å¥¥ä¼š#  #å†¬å¥¥...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/DVYNperQlx07TY...</td>\n",
       "      <td>285</td>\n",
       "      <td>1009</td>\n",
       "      <td>15334</td>\n",
       "      <td>#å†¬å¥¥ä¼š# #å†¬å¥¥éš”ç¦»æ—¥è®°#</td>\n",
       "      <td>./video/4.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>syç‹è¯—ç¥</td>\n",
       "      <td>2022-02-23 14:21:00.000001</td>\n",
       "      <td>éš”ç¦»æœŸé—´ä¹Ÿè¦åŠ¨èµ·æ¥ï½  å¤§å®¶è·Ÿæˆ‘ä¸€èµ·é”»ç‚¼å§ï½ğŸ¤¸ğŸ»â€â™€ï¸   #å†¬å¥¥ä¼š#  #å†¬å¥¥éš”ç¦»æ—¥è®°# ...</td>\n",
       "      <td>https://f.video.weibocdn.com/o0/E5AJHWkMlx07TY...</td>\n",
       "      <td>288</td>\n",
       "      <td>978</td>\n",
       "      <td>13871</td>\n",
       "      <td>#å†¬å¥¥ä¼š# #å†¬å¥¥éš”ç¦»æ—¥è®°#</td>\n",
       "      <td>./video/5.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Username              Post_datetime  \\\n",
       "0  Ice-danceæŸ³é‘«å®‡ 2022-03-05 16:10:00.000000   \n",
       "1       å¥¥æ—åŒ¹å…‹è¿åŠ¨ä¼š 2022-03-16 12:30:00.000001   \n",
       "2       å¥¥æ—åŒ¹å…‹è¿åŠ¨ä¼š 2022-03-18 08:00:00.000001   \n",
       "3  Ice-danceæŸ³é‘«å®‡ 2022-02-23 14:21:00.000001   \n",
       "4         syç‹è¯—ç¥ 2022-02-23 14:21:00.000001   \n",
       "\n",
       "                                             Content  \\\n",
       "0  æˆ‘ç”¨äº†æˆ‘å¾ˆå–œæ¬¢çš„ä¸€é¦–æ­Œæ›²æ¥å½“èƒŒæ™¯éŸ³ä¹ï¼Œæ¥ä¸å¤§å®¶åˆ†äº«æˆ‘çš„é—­å¹•å¼vlogï¼ŒåŒ—äº¬å†¬å¥¥ä¼šçœŸçš„ç»“æŸäº†ï¼Œ...   \n",
       "1  ä¸€èµ·æ¬£èµ @é‡‘åšæ´‹çš„å¤©å¤©  åœ¨åŒ—äº¬2022å¹´å†¬å¥¥ä¼šä¸Šçš„çŸ­èŠ‚ç›®è¡¨æ¼”   #å¥¥è¿ä¼š#  ï½œ  #...   \n",
       "2  ä¸€èµ·å›é¡¾ #ç¾½ç”Ÿç»“å¼¦# çš„å†¬å¥¥ä¹‹æ—…   #å¥¥è¿ä¼š#  ï½œ #åŒ—äº¬2022å¹´å†¬å¥¥ä¼š#    L...   \n",
       "3  éš”ç¦»ä¸­ä¹Ÿè¦è¿åŠ¨èµ·æ¥å“¦ï¼ è®­ç»ƒè®¡åˆ’å®‰æ’ï¼ å¤§å®¶å¿«è·Ÿæˆ‘ä¸€èµ·æ“ç»ƒèµ·æ¥å§ï¼   #å†¬å¥¥ä¼š#  #å†¬å¥¥...   \n",
       "4  éš”ç¦»æœŸé—´ä¹Ÿè¦åŠ¨èµ·æ¥ï½  å¤§å®¶è·Ÿæˆ‘ä¸€èµ·é”»ç‚¼å§ï½ğŸ¤¸ğŸ»â€â™€ï¸   #å†¬å¥¥ä¼š#  #å†¬å¥¥éš”ç¦»æ—¥è®°# ...   \n",
       "\n",
       "                                 Video_link(expired)  Repost_Count  \\\n",
       "0  https://f.video.weibocdn.com/o0/b7tMvrhxlx07Uf...           939   \n",
       "1  https://f.video.weibocdn.com/o0/Due4Y9s7lx07Uw...           852   \n",
       "2  https://f.video.weibocdn.com/o0/sJtalMgylx07Uy...          3565   \n",
       "3  https://f.video.weibocdn.com/o0/DVYNperQlx07TY...           285   \n",
       "4  https://f.video.weibocdn.com/o0/E5AJHWkMlx07TY...           288   \n",
       "\n",
       "   Comment_Count  Like_Count                   Keywords     Video_file  \n",
       "0           1532       29070             #å†¬å¥¥ä¼š# #å†¬å¥¥éš”ç¦»æ—¥è®°#  ./video/1.mp4  \n",
       "1            305        2561         #å¥¥è¿ä¼š# #åŒ—äº¬2022å¹´å†¬å¥¥ä¼š#  ./video/2.mp4  \n",
       "2            851       19006  #ç¾½ç”Ÿç»“å¼¦# #å¥¥è¿ä¼š# #åŒ—äº¬2022å¹´å†¬å¥¥ä¼š#  ./video/3.mp4  \n",
       "3           1009       15334             #å†¬å¥¥ä¼š# #å†¬å¥¥éš”ç¦»æ—¥è®°#  ./video/4.mp4  \n",
       "4            978       13871             #å†¬å¥¥ä¼š# #å†¬å¥¥éš”ç¦»æ—¥è®°#  ./video/5.mp4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_excel('./data.xlsx')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify frequent phrases of differene length\n",
    "def n_words_count(data, n):\n",
    "        '''\n",
    "        Return number of occurrence of words with length n.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data: List\n",
    "            A list sentences or paragraphs\n",
    "        \n",
    "        n: int\n",
    "            return the words with length n\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Dict:\n",
    "            A dictionary mapping words with their frequency\n",
    "        '''\n",
    "        count_dict = defaultdict(lambda: 0)\n",
    "        for sentence in data:\n",
    "            \n",
    "            for ptr1 in range(len(sentence)-n):\n",
    "                ptr2 = ptr1 + n\n",
    "                phrases = sentence[ptr1:ptr2]\n",
    "                count_dict[phrases]+=1\n",
    "        pairs = list(count_dict.items())\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])[::-1]\n",
    "        return pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output frequency of word phrases with length 2 to 10\n",
    "# Used to select meaningful frequent word phrase\n",
    "data = list(df_all['Content'].values)\n",
    "unwanted = re.compile('[!ï¼Œ?#ï¼šï¼ˆï¼‰ã€‚ã€Šã€‹ã€@â€¦â€¦&â€œâ€ï¼ã€ã€‘ï½œ....____â†“ï¼Ÿ ]')\n",
    "data = [unwanted.sub('', s) for s in data]\n",
    "for word_l in range(2, 11):\n",
    "    words_count = n_words_count(data, word_l)\n",
    "    df = pd.DataFrame(words_count, columns=['words','count'])\n",
    "    df.to_excel('./data/word_count_tables/wc{}.xlsx'.format(word_l), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>æ®‹å¥¥</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>å¼€å¹•</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>è·³å°æ»‘é›ª</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>é‡‘ç‰Œ</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>é—­å¹•</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words  count\n",
       "0    æ®‹å¥¥    991\n",
       "1    å¼€å¹•    265\n",
       "2  è·³å°æ»‘é›ª    116\n",
       "3    é‡‘ç‰Œ    183\n",
       "4    é—­å¹•    179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words = pd.read_excel(r'./data/freq_words.xlsx')\n",
    "freq_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Username: Ice-danceæŸ³é‘«å®‡', 'é—­å¹•'}, {'Username: å¥¥æ—åŒ¹å…‹è¿åŠ¨ä¼š'}, {'Username: å¥¥æ—åŒ¹å…‹è¿åŠ¨ä¼š', 'ç¾½ç”Ÿç»“å¼¦'}, {'Username: Ice-danceæŸ³é‘«å®‡'}, {'Username: syç‹è¯—ç¥'}]\n"
     ]
    }
   ],
   "source": [
    "# Turn each post into a bag of features so that it can be passed to Apriori \n",
    "data = []\n",
    "for idx, row in df_all.iterrows():\n",
    "    bag = set()\n",
    "    bag.add('Username: '+row['Username'])\n",
    "    if not row['Keywords']:\n",
    "        keywords = {kw for kw in row['Keywords'].split()}\n",
    "        bag.update(keywords)\n",
    "    text = row['Content']\n",
    "    for interested_word in freq_words['words']:\n",
    "        if interested_word in text:\n",
    "            bag.add(interested_word)\n",
    "    data.append(bag)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: ('å†°ç«‹æ–¹',) -> ('å†°å£¶',)  Support: 0.02185792349726776  Confidence: 0.7058823529411765\n",
      "Rule: ('é›ªè½¦é›ªæ©‡',) -> ('å»¶åº†',)  Support: 0.026411657559198543  Confidence: 0.6444444444444445\n",
      "Rule: ('çŸ­é“é€Ÿæ»‘',) -> ('Username: åŒ—äº¬2022å¹´å†¬å¥¥ä¼š',)  Support: 0.033697632058287796  Confidence: 0.6379310344827587\n",
      "Rule: ('äººæ°‘æ—¥æŠ¥',) -> ('Username: äººæ°‘æ—¥æŠ¥',)  Support: 0.02185792349726776  Confidence: 0.6\n",
      "Rule: ('é€Ÿåº¦æ»‘å†°',) -> ('Username: åŒ—äº¬2022å¹´å†¬å¥¥ä¼š',)  Support: 0.03278688524590164  Confidence: 0.6792452830188679\n",
      "Rule: ('å†°ä¸å¸¦',) -> ('é€Ÿåº¦æ»‘å†°',)  Support: 0.02185792349726776  Confidence: 0.6666666666666666\n",
      "Rule: ('é—­å¹•',) -> ('æ®‹å¥¥',)  Support: 0.05009107468123861  Confidence: 0.6179775280898876\n",
      "Rule: ('é¸Ÿå·¢',) -> ('å¼€å¹•',)  Support: 0.030054644808743168  Confidence: 0.7333333333333333\n",
      "Rule: ('Username: å¤®è§†é¢‘',) -> ('æ®‹å¥¥',)  Support: 0.033697632058287796  Confidence: 0.6379310344827587\n",
      "Rule: ('é›ªå®¹è',) -> ('æ®‹å¥¥',)  Support: 0.061930783242258654  Confidence: 0.7816091954022989\n",
      "Rule: ('å†°å£¶',) -> ('æ®‹å¥¥',)  Support: 0.04918032786885246  Confidence: 0.6352941176470588\n",
      "Rule: ('äº‘é¡¶æ»‘é›ªå…¬å›­',) -> ('å¼ å®¶å£',)  Support: 0.02459016393442623  Confidence: 0.675\n",
      "Rule: ('å›½å®¶é€Ÿæ»‘é¦†',) -> ('å†°ä¸å¸¦',)  Support: 0.023679417122040074  Confidence: 0.7428571428571429\n",
      "Rule: ('å†°ä¸å¸¦',) -> ('å›½å®¶é€Ÿæ»‘é¦†',)  Support: 0.023679417122040074  Confidence: 0.7222222222222222\n",
      "Rule: ('3æœˆ4æ—¥',) -> ('å¼€å¹•',)  Support: 0.030054644808743168  Confidence: 0.7021276595744681\n",
      "Rule: ('3æœˆ4æ—¥',) -> ('æ®‹å¥¥',)  Support: 0.042805100182149364  Confidence: 1.0\n",
      "Rule: ('å•æ¿æ»‘é›ª', 'é«˜å±±æ»‘é›ª') -> ('æ®‹å¥¥',)  Support: 0.02185792349726776  Confidence: 0.6\n",
      "Rule: ('çŸ­é“é€Ÿæ»‘', 'é‡‘ç‰Œ') -> ('Username: åŒ—äº¬2022å¹´å†¬å¥¥ä¼š',)  Support: 0.020036429872495445  Confidence: 0.88\n",
      "Rule: ('è¶Šé‡æ»‘é›ª', 'å•æ¿æ»‘é›ª') -> ('æ®‹å¥¥',)  Support: 0.023679417122040074  Confidence: 0.7428571428571429\n",
      "Rule: ('æ®‹å¥¥', 'å†°çƒ') -> ('å•æ¿æ»‘é›ª',)  Support: 0.030054644808743168  Confidence: 0.673469387755102\n",
      "Rule: ('æ®‹å¥¥', 'å•æ¿æ»‘é›ª') -> ('å†°çƒ',)  Support: 0.030054644808743168  Confidence: 0.66\n",
      "Rule: ('å†°çƒ', 'å•æ¿æ»‘é›ª') -> ('æ®‹å¥¥',)  Support: 0.030054644808743168  Confidence: 1.0\n",
      "Rule: ('å†°çƒ', 'è¶Šé‡æ»‘é›ª') -> ('æ®‹å¥¥',)  Support: 0.020947176684881604  Confidence: 1.0\n",
      "Rule: ('å†°çƒ', 'å•æ¿æ»‘é›ª') -> ('é«˜å±±æ»‘é›ª',)  Support: 0.02185792349726776  Confidence: 0.7272727272727273\n",
      "Rule: ('å†°çƒ', 'é«˜å±±æ»‘é›ª') -> ('å•æ¿æ»‘é›ª',)  Support: 0.02185792349726776  Confidence: 0.96\n",
      "Rule: ('å•æ¿æ»‘é›ª', 'é«˜å±±æ»‘é›ª') -> ('å†°çƒ',)  Support: 0.02185792349726776  Confidence: 0.6\n",
      "Rule: ('3æœˆ4æ—¥', 'å†°çƒ') -> ('æ®‹å¥¥',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('3æœˆ4æ—¥', 'å•æ¿æ»‘é›ª') -> ('æ®‹å¥¥',)  Support: 0.023679417122040074  Confidence: 1.0\n",
      "Rule: ('æ®‹å¥¥', 'å†°å¢©å¢©') -> ('é›ªå®¹è',)  Support: 0.0273224043715847  Confidence: 1.0\n",
      "Rule: ('å†°å¢©å¢©', 'é›ªå®¹è') -> ('æ®‹å¥¥',)  Support: 0.0273224043715847  Confidence: 0.6976744186046512\n",
      "Rule: ('3æœˆ4æ—¥', 'å†°å£¶') -> ('æ®‹å¥¥',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('å†°å£¶', 'å†°çƒ') -> ('é«˜å±±æ»‘é›ª',)  Support: 0.020036429872495445  Confidence: 0.6470588235294118\n",
      "Rule: ('å†°å£¶', 'é«˜å±±æ»‘é›ª') -> ('å†°çƒ',)  Support: 0.020036429872495445  Confidence: 0.9565217391304348\n",
      "Rule: ('å†°çƒ', 'é«˜å±±æ»‘é›ª') -> ('å†°å£¶',)  Support: 0.020036429872495445  Confidence: 0.88\n",
      "Rule: ('Username: åŒ—äº¬2022å¹´å†¬å¥¥ä¼š', 'é›ªå®¹è') -> ('æ®‹å¥¥',)  Support: 0.020947176684881604  Confidence: 0.9583333333333334\n",
      "Rule: ('3æœˆ4æ—¥', 'å†°å£¶') -> ('å†°çƒ',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('3æœˆ4æ—¥', 'å†°çƒ') -> ('å†°å£¶',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('å†°å£¶', 'å†°çƒ') -> ('3æœˆ4æ—¥',)  Support: 0.02459016393442623  Confidence: 0.7941176470588235\n",
      "Rule: ('è¶Šé‡æ»‘é›ª', 'å•æ¿æ»‘é›ª') -> ('é«˜å±±æ»‘é›ª',)  Support: 0.02459016393442623  Confidence: 0.7714285714285715\n",
      "Rule: ('è¶Šé‡æ»‘é›ª', 'é«˜å±±æ»‘é›ª') -> ('å•æ¿æ»‘é›ª',)  Support: 0.02459016393442623  Confidence: 0.84375\n",
      "Rule: ('å•æ¿æ»‘é›ª', 'é«˜å±±æ»‘é›ª') -> ('è¶Šé‡æ»‘é›ª',)  Support: 0.02459016393442623  Confidence: 0.675\n",
      "Rule: ('3æœˆ4æ—¥', 'å†°çƒ') -> ('å•æ¿æ»‘é›ª',)  Support: 0.023679417122040074  Confidence: 0.9629629629629629\n",
      "Rule: ('3æœˆ4æ—¥', 'å•æ¿æ»‘é›ª') -> ('å†°çƒ',)  Support: 0.023679417122040074  Confidence: 1.0\n",
      "Rule: ('å†°çƒ', 'å•æ¿æ»‘é›ª') -> ('3æœˆ4æ—¥',)  Support: 0.023679417122040074  Confidence: 0.7878787878787878\n",
      "Rule: ('3æœˆ4æ—¥', 'å†°å£¶') -> ('å•æ¿æ»‘é›ª',)  Support: 0.023679417122040074  Confidence: 0.9629629629629629\n",
      "Rule: ('3æœˆ4æ—¥', 'å•æ¿æ»‘é›ª') -> ('å†°å£¶',)  Support: 0.023679417122040074  Confidence: 1.0\n",
      "Rule: ('å†°å£¶', 'å•æ¿æ»‘é›ª') -> ('3æœˆ4æ—¥',)  Support: 0.023679417122040074  Confidence: 0.9629629629629629\n",
      "Rule: ('å†°å£¶', 'å†°çƒ') -> ('å•æ¿æ»‘é›ª',)  Support: 0.02459016393442623  Confidence: 0.7941176470588235\n",
      "Rule: ('å†°å£¶', 'å•æ¿æ»‘é›ª') -> ('å†°çƒ',)  Support: 0.02459016393442623  Confidence: 1.0\n",
      "Rule: ('å†°çƒ', 'å•æ¿æ»‘é›ª') -> ('å†°å£¶',)  Support: 0.02459016393442623  Confidence: 0.8181818181818182\n",
      "Rule: ('è¶Šé‡æ»‘é›ª', 'é«˜å±±æ»‘é›ª') -> ('æ®‹å¥¥',)  Support: 0.020947176684881604  Confidence: 0.71875\n",
      "Rule: ('å†°çƒ', 'é«˜å±±æ»‘é›ª') -> ('æ®‹å¥¥',)  Support: 0.02185792349726776  Confidence: 0.96\n"
     ]
    }
   ],
   "source": [
    "model = apriori(min_support=0.02, min_confidence=0.6)\n",
    "rules = model.find_rules(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not all rules are meaningful, some contain interesting facts. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule: ('é›ªå®¹è',) -> ('æ®‹å¥¥',)  Support: 0.061930783242258654  Confidence: 0.7816091954022989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule relates the name of mascot ('é›ªå®¹è') to Winter Paralympic Games ('æ®‹å¥¥'). The confidence value tells that given that the word 'é›ªå®¹è' appears in the post content, there is around 78.16% of chance that the word 'æ®‹å¥¥' will also appear in the same post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
